{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16c5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394852c2",
   "metadata": {},
   "source": [
    "## Create pub/sub topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8702b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"kubeflow-1-0-2\"\n",
    "topic_id = \"inference_images\"\n",
    "subscription_id = \"my_subscription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c290fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic [projects/kubeflow-1-0-2/topics/inference_images].\n"
     ]
    }
   ],
   "source": [
    "!gcloud pubsub topics create $topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056debcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_settings = pubsub_v1.types.BatchSettings(\n",
    "                                               max_messages=3,  # default 100\n",
    "                                               max_bytes=1024*1024,  # default 1 MiB\n",
    "                                               max_latency=2,  # seconds; default 10 ms\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2324a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher = pubsub_v1.PublisherClient(batch_settings)\n",
    "topic_path = publisher.topic_path(project_id, topic_id)\n",
    "\n",
    "# publish_futures = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ec56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_callback(future: pubsub_v1.publisher.futures.Future) -> None:\n",
    "    message_id = future.result()\n",
    "    print(message_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eeb50d",
   "metadata": {},
   "source": [
    "**Note**: At this stage, before running the following cells, move to the other notebook, in order to create subscription to this topic, so that messages published from hereon, can be received later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037d1d2",
   "metadata": {},
   "source": [
    "## Publish image files (as messages) to a pub/sub topic (for buffering; load-balancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96174e78",
   "metadata": {},
   "source": [
    "Get list of images for which inferencing is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51e88f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://fire_detection_anurag/test_images/fire1.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire2.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire3.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire4.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire5.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire1.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire2.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire3.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire4.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire5.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = tf.io.gfile.glob(\"gs://fire_detection_anurag/test_images/*\")\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55d10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs of messages published...\n",
      "3472774071426909\n",
      "3472774071426910\n",
      "3472778815427077\n",
      "3472778815427078\n",
      "3472774478471355\n"
     ]
    }
   ],
   "source": [
    "print(\"IDs of messages published...\")\n",
    "\n",
    "for file in filenames[:5]:\n",
    "    # Data must be a bytestring\n",
    "    data = file.encode(\"utf-8\")\n",
    "    publish_future = publisher.publish(topic_path, data)\n",
    "    # Non-blocking. Allow the publisher client to batch multiple messages.\n",
    "    publish_future.add_done_callback(pub_callback)\n",
    "#     publish_futures.append(publish_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517cb51",
   "metadata": {},
   "source": [
    "Now run the other notebook (or/and python script) for 1. receving messages through asynchronous pull and 2. inferencing with Apache Beam (for auto-scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f61aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs of 2nd lot of messages published...\n",
      "3472737478205979\n",
      "3472737478205980\n",
      "3472785780480443\n",
      "3472785780480444\n",
      "3472794187459134\n"
     ]
    }
   ],
   "source": [
    "print(\"IDs of 2nd lot of messages published...\")\n",
    "\n",
    "for file in filenames[5:]:\n",
    "    # Data must be a bytestring\n",
    "    data = file.encode(\"utf-8\")\n",
    "    publish_future = publisher.publish(topic_path, data)\n",
    "    # Non-blocking. Allow the publisher client to batch multiple messages.\n",
    "    publish_future.add_done_callback(pub_callback)\n",
    "#     publish_futures.append(publish_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38061e53",
   "metadata": {},
   "source": [
    "**Note**: Since the subscriber (other notebook) was continuously listening, it keeps waiting for new messages (images) to be streamed in, and Apache Beam takes care of the scaling up/down (while inferencing) accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e920ea",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e90a21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted topic [projects/kubeflow-1-0-2/topics/inference_images].\n"
     ]
    }
   ],
   "source": [
    "!gcloud pubsub topics delete $topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d51e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
