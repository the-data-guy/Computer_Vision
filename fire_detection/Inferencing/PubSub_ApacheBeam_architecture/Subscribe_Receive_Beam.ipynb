{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c50018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# import json\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# import requests\n",
    "\n",
    "# from google.cloud import pubsub_v1\n",
    "\n",
    "# import apache_beam as beam\n",
    "# from apache_beam.options.pipeline_options import PipelineOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f147040",
   "metadata": {},
   "source": [
    "Look at list of image-files which will be sent via pub/sub messaging, for inferencing/prediction.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520de5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://fire_detection_anurag/test_images/fire1.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire2.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire3.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire4.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/fire5.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire1.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire2.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire3.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire4.jpg',\n",
       " 'gs://fire_detection_anurag/test_images/no_fire5.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = tf.io.gfile.glob(\"gs://fire_detection_anurag/test_images/*\")\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b15a59",
   "metadata": {},
   "source": [
    "## Subscribe to the pub/sub topic (created in other notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8211ca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/kubeflow-1-0-2/topics/inference_images',\n",
       " 'projects/kubeflow-1-0-2/subscriptions/my_subscription')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = \"kubeflow-1-0-2\"\n",
    "\n",
    "topic_id = \"inference_images\"  # created in other notebook\n",
    "pub_sub_topic = \"projects/{}/topics/{}\".format(project_id, topic_id)\n",
    "\n",
    "subscription_id = \"my_subscription\"\n",
    "subscription_id = \"projects/{}/subscriptions/{}\".format(project_id, subscription_id)\n",
    "\n",
    "pub_sub_topic, subscription_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128663d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created subscription [projects/kubeflow-1-0-2/subscriptions/my_subscription].\n"
     ]
    }
   ],
   "source": [
    "!gcloud pubsub subscriptions create $subscription_id --topic $topic_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cd80b",
   "metadata": {},
   "source": [
    "## Connect pub/sub topic to Apache Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90706e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pub_sub_beam.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pub_sub_beam.py\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import json\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import requests\n",
    "\n",
    "pub_sub_topic = 'projects/kubeflow-1-0-2/topics/inference_images'\n",
    "subscription_id = 'projects/kubeflow-1-0-2/subscriptions/my_subscription'\n",
    "PROJECT = \"kubeflow-1-0-2\"\n",
    "REGION = \"us-central1\"\n",
    "ENDPOINT_ID = \"6569494015031377920\"\n",
    "\n",
    "class ModelPredict:\n",
    "    def __init__(self, project, region, endpoint_id):\n",
    "        self._api = \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:predict\".format(\n",
    "                                                                                                                   region,\n",
    "                                                                                                                   project,\n",
    "                                                                                                                   region,\n",
    "                                                                                                                   endpoint_id\n",
    "                                                                                                                  )   \n",
    "        \n",
    "    def __call__(self, filenames):        \n",
    "        token = GoogleCredentials.get_application_default().get_access_token().access_token\n",
    "        if isinstance(filenames, str):\n",
    "            # Only one element, put it into a batch of 1.\n",
    "            data = {\n",
    "                    \"instances\": [\n",
    "                                  {\"filenames\": filenames}\n",
    "                                 ]\n",
    "                   }\n",
    "        else:\n",
    "            data = {\n",
    "                    \"instances\": []\n",
    "                   }\n",
    "            for f in filenames:\n",
    "                data[\"instances\"].append({\n",
    "                                          \"filenames\" : f\n",
    "                                        })\n",
    "        # print(data)\n",
    "        headers = {\"Authorization\": \"Bearer \" + token }\n",
    "        response = requests.post(self._api,\n",
    "                                 json=data,\n",
    "                                 headers=headers)\n",
    "        response = json.loads(response.content.decode(\"utf-8\"))\n",
    "        \n",
    "        if isinstance(filenames, str):\n",
    "            result = response[\"predictions\"][0]\n",
    "            result[\"filename\"] = filenames\n",
    "            yield result\n",
    "        else:\n",
    "            for (a,b) in zip(filenames, response[\"predictions\"]):\n",
    "                result = b\n",
    "                result[\"filename\"] = a\n",
    "                yield result\n",
    "\n",
    "pipeline_options = PipelineOptions(\n",
    "                                   streaming=True,  # required for Beam connector with pub/sub\n",
    "                                  )\n",
    "                \n",
    "with beam.Pipeline(options=pipeline_options) as p:\n",
    "    (p \n",
    "     | \"getinput\" >> beam.io.ReadFromPubSub(subscription=subscription_id)\n",
    "     | \"batch\" >> beam.BatchElements(min_batch_size=2,\n",
    "                                     max_batch_size=3)\n",
    "     | \"getpred\" >> beam.FlatMap(ModelPredict(PROJECT,\n",
    "                                              REGION,\n",
    "                                              ENDPOINT_ID))\n",
    "     | \"write\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd52321",
   "metadata": {},
   "source": [
    "## Inferencing on streaming (or batch of) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c0b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.994521737, 'image_type_str': 'Fire', 'image_type_int': 0, 'filename': b'gs://fire_detection_anurag/test_images/fire1.jpg'}\n",
      "{'image_type_int': 0, 'probability': 0.711659372, 'image_type_str': 'Fire', 'filename': b'gs://fire_detection_anurag/test_images/fire2.jpg'}\n",
      "{'probability': 0.987360537, 'image_type_str': 'Fire', 'image_type_int': 0, 'filename': b'gs://fire_detection_anurag/test_images/fire3.jpg'}\n",
      "{'image_type_str': 'Fire', 'image_type_int': 0, 'probability': 0.983968496, 'filename': b'gs://fire_detection_anurag/test_images/fire4.jpg'}\n",
      "{'image_type_str': 'Fire', 'image_type_int': 0, 'probability': 0.995875299, 'filename': b'gs://fire_detection_anurag/test_images/fire5.jpg'}\n",
      "{'image_type_int': 1, 'probability': 0.827532887, 'image_type_str': 'No-Fire', 'filename': b'gs://fire_detection_anurag/test_images/no_fire1.jpg'}\n",
      "{'image_type_int': 1, 'probability': 0.990039051, 'image_type_str': 'No-Fire', 'filename': b'gs://fire_detection_anurag/test_images/no_fire2.jpg'}\n",
      "{'image_type_str': 'No-Fire', 'image_type_int': 1, 'probability': 0.984749556, 'filename': b'gs://fire_detection_anurag/test_images/no_fire3.jpg'}\n",
      "{'image_type_int': 1, 'probability': 0.981689095, 'image_type_str': 'No-Fire', 'filename': b'gs://fire_detection_anurag/test_images/no_fire4.jpg'}\n",
      "{'image_type_int': 1, 'probability': 0.993100286, 'image_type_str': 'No-Fire', 'filename': b'gs://fire_detection_anurag/test_images/no_fire5.jpg'}\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"pub_sub_beam.py\", line 68, in <module>\n",
      "    | \"write\" >> beam.Map(print)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py\", line 581, in __exit__\n",
      "    self.result.wait_until_finish()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/apache_beam/runners/direct/direct_runner.py\", line 590, in wait_until_finish\n",
      "    self._executor.await_completion()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/apache_beam/runners/direct/executor.py\", line 439, in await_completion\n",
      "    self._executor.await_completion()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/apache_beam/runners/direct/executor.py\", line 484, in await_completion\n",
      "    update = self.visible_updates.take()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/apache_beam/runners/direct/executor.py\", line 559, in take\n",
      "    item = self._queue.get(timeout=1)\n",
      "  File \"/opt/conda/lib/python3.7/queue.py\", line 179, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 300, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python pub_sub_beam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b50a4c",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451c16cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted subscription [projects/kubeflow-1-0-2/subscriptions/my_subscription].\n"
     ]
    }
   ],
   "source": [
    "!gcloud pubsub subscriptions delete $subscription_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9ce57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
